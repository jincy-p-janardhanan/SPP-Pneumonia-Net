{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "SPP-Pneumonia-Net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "pGvayTWBx9Ff",
        "iaqHU2Aya-NX",
        "waRhasFrT3Lo",
        "J5PjuHC3gHGU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jincy-p-janardhanan/SPP-Pneumonia-Net/blob/ml/SPP_Pneumonia_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJY2rkKr5qB"
      },
      "source": [
        "Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTpzq6Gkr6_P",
        "outputId": "69047501-d58e-4020-ffcd-658b4c99bc20"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGvayTWBx9Ff"
      },
      "source": [
        "# Dependencies & Constants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLo1fjYnx54W"
      },
      "source": [
        "Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIOZJU8Rxqhx"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import h5py\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.client import device_lib\n",
        "from random import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDdQmPDaHr2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795a7c92-e18d-4ea8-8dd3-f481f0894f36"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 30 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 40 kB 14.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 61 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 71 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 81 kB 6.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 92 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.41.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.4)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner) (3.7.4.3)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.0.4 kt-legacy-1.0.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odFTk71IH3nZ"
      },
      "source": [
        "import keras_tuner as kt"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeSscahixxGy",
        "outputId": "f65cb518-5fb9-4964-d755-66f4c0e3f675"
      },
      "source": [
        "print(device_lib.list_local_devices())\n",
        "print(tf.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 8191562639169356211\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 11345264640\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 11543936405937431176\n",
            "physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n",
            "]\n",
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCDypOWanew-"
      },
      "source": [
        "Image shape parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkynRPdgm7aC"
      },
      "source": [
        "input_height = 128\n",
        "input_width = 128\n",
        "n_channels = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W2AaTrSniWP"
      },
      "source": [
        "Class labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpRSOEejnkr7"
      },
      "source": [
        "class_no=3\n",
        "labels = {0:'Normal', 1:'Bacterial', 2:'Viral'}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbwi9CByoUAr"
      },
      "source": [
        "To save models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YibBJNYtonfe"
      },
      "source": [
        "model_path = '/content/drive/MyDrive/main_project/models/saved_model_'\n",
        "\n",
        "# Last saved model number\n",
        "model_count = 80"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHSB_F9NQKZk"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLgkfpxJrnlK"
      },
      "source": [
        "The hp parameter of the model helps in tuning hyperparameters using keras_tuner. <br>\n",
        "[Keras Tuner Reference](https://www.tensorflow.org/tutorials/keras/keras_tuner) <br>\n",
        "[Model Reference](https://github.com/SitiRaihanah/SPP-COVID-Net/blob/master/SPP-COVID-Net.py)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Mn54aH-QT2L"
      },
      "source": [
        "def SPPPneumoniaNet(hp):\n",
        "\n",
        "    input_images=tf.keras.layers.Input(shape=(input_height,input_width,n_channels))\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(hp.Int('layer1_units', min_value=4, max_value=8, step=1),\n",
        "                               (3,3),strides=(1,1),padding='same', use_bias=False)(input_images)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2,2),strides=(2,2))(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(hp.Int('layer2_units', min_value=4, max_value=16, step=1),\n",
        "                               (3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2,2),strides=(2,2))(x)\n",
        "\n",
        "    #first triple\n",
        "    n1 = hp.Int('triple1_units', min_value=32, max_value=64, step=1)\n",
        "    x = tf.keras.layers.Conv2D(n1,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n1//2,(1,1),strides=(1,1),padding='same',use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n1,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2,2),strides=(2,2))(x)\n",
        "\n",
        "    #second triple\n",
        "    n2 = hp.Int('triple2_units', min_value=64, max_value=128, step=1)\n",
        "    x = tf.keras.layers.Conv2D(n2,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n2//2,(1,1),strides=(1,1),padding='same',use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n2,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2,2),strides=(2,2))(x)\n",
        "\n",
        "    #third triple\n",
        "    n3 = hp.Int('triple3_units', min_value=128, max_value=256, step=1)\n",
        "    x = tf.keras.layers.Conv2D(n3,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n3//2,(1,1),strides=(1,1),padding='same',use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n3,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "    x = tf.keras.layers.MaxPooling2D((2,2),strides=(2,2))(x)\n",
        "\n",
        "    #fourth triple\n",
        "    n4 = hp.Int('triple4_units', min_value=256, max_value=512, step=32)\n",
        "    x = tf.keras.layers.Conv2D(n4,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n4//2,(1,1),strides=(1,1),padding='same',use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    x = tf.keras.layers.Conv2D(n4,(3,3),strides=(1,1),padding='same', use_bias=False)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "    #ending network\n",
        "    L1 = tf.keras.layers.MaxPooling2D((4, 4),strides=(1,1),padding='valid')(x)\n",
        "    L2 = tf.keras.layers.MaxPooling2D((3, 3),strides=(1,1),padding='valid')(x)\n",
        "    L3 = tf.keras.layers.MaxPooling2D((2, 2),strides=(1,1),padding='valid')(x)\n",
        "\n",
        "    FL1 = tf.keras.layers.Flatten()(L1)\n",
        "    FL2 = tf.keras.layers.Flatten()(L2)\n",
        "    FL3 = tf.keras.layers.Flatten()(L3)\n",
        "    \n",
        "    x = tf.keras.layers.Concatenate(axis=1)([FL1,FL2,FL3])\n",
        "    x = tf.keras.layers.Dense(class_no,activation='softmax')(x)\n",
        "\n",
        "    # Create model.\n",
        "    model=tf.keras.models.Model(inputs=input_images,outputs=x)\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adagrad(\n",
        "        hp.Float('learning_rate', 1e-4, 1e-1, sampling='log'),\n",
        "        hp.Float('initial_accumulator_value', 1e-4, 1e-1, sampling='log'),\n",
        "        hp.Float('epsilon', 1e-07, 1e-04, sampling='log')), \n",
        "        loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqHU2Aya-NX"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzXfxvTxsMxD"
      },
      "source": [
        "Load complete dataset from h5 file to arrays x and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBI9JfIBnOMt"
      },
      "source": [
        "dest_filepath = '/content/drive/MyDrive/main_project/dataseth5/complete_dataseth5_1.h5'\n",
        "\n",
        "with h5py.File(dest_filepath, \"r\") as f:\n",
        "    x = f[\"input_data\"][:]\n",
        "    y = f[\"input_labels\"][:]\n",
        "f.close()\n",
        "x = np.reshape(x, (x.shape[0], 128, 128, 1))\n",
        "print('x shape =', x.shape, '| y shape =', y.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQij3zUKmpOd"
      },
      "source": [
        "## Preview Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLg7z7Etm0Vy"
      },
      "source": [
        "Plot some images from the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCJ88mCmrdpQ"
      },
      "source": [
        "plt.figure(figsize=(15,10))\n",
        "for i in range(3):\n",
        "    for j in range(5):\n",
        "        idx = 5*i+j\n",
        "        label_no = y[idx]\n",
        "        plt.subplot(3,5,idx+1)\n",
        "        im = np.squeeze(x[idx])\n",
        "        plt.imshow(im)\n",
        "        plt.title(labels[label_no])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWXWTh_grBDZ"
      },
      "source": [
        "## Train, validation & test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YcnHSBVc2qo"
      },
      "source": [
        "train, validation and test split using h5file and sklearn <br>\n",
        "[reference](https://www.machinecurve.com/index.php/2020/11/16/how-to-easily-create-a-train-test-split-for-your-machine-learning-model/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xLlHyIVdAqp"
      },
      "source": [
        "> Shuffling (i.e. randomly drawing) samples is applied as part of the fit. Using a random_state, we can seed the random number generator to make its behavior replicable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucanVFD9Coek"
      },
      "source": [
        "random_state = 55\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=random_state)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=0.5, random_state=random_state)\n",
        "print('train size: ', len(y_train), '\\t val size: ', len(y_val), '\\t test size: ', len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJsToimFrO8j"
      },
      "source": [
        "## One-hot encoding / Categorical encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVZPhUrIpZ2u"
      },
      "source": [
        "One hot encoding for y_train and y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hx19xANnoMGz"
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train, 3)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, 3)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzUtrnKpr7GE"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHqS981JA1DW"
      },
      "source": [
        "datagen = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.1, fill_mode='nearest')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiNs8gGYwhCO"
      },
      "source": [
        "# Hyperparameter Tuning Using Keras Tuner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-izqgciZ45tm"
      },
      "source": [
        "[Reference](https://www.tensorflow.org/tutorials/keras/keras_tuner)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMbrZNM2_H7u"
      },
      "source": [
        "Create a hyperband tuner instance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAyVgbDwIL9n"
      },
      "source": [
        "tuner = kt.Hyperband(SPPPneumoniaNet, objective='val_accuracy', max_epochs=15, hyperband_iterations=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0aW7_c8_Lhq"
      },
      "source": [
        "Early stopping callback to stop training if val_loss is not improving within 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPAsWefoNMpU"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D62VX5tC_iw0"
      },
      "source": [
        "Hyperparameter search using hyperband tuner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2NSFzMPKzqc"
      },
      "source": [
        "tuner.search(datagen.flow(x_train, y_train, batch_size=4), steps_per_epoch=x_train.shape[0],\n",
        "                      verbose=2, validation_data=(x_val, y_val), callbacks=[stop_early], workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiGLrnEJ_ter"
      },
      "source": [
        "Retrieve the best hyperparameters obtained"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J9syJZtNbXr"
      },
      "source": [
        "best_hyperparameters = tuner.get_best_hyperparameters(1)[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXAfbRbs_5le"
      },
      "source": [
        "Best hyperparameters obtained "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrD3zyR4aU-U"
      },
      "source": [
        "print('layer1_units: ', best_hyperparameters.get('layer1_units'))\n",
        "print('layer2_units: ', best_hyperparameters.get('layer2_units'))\n",
        "print('triple1_units: ', best_hyperparameters.get('triple1_units'))\n",
        "print('triple2_units: ', best_hyperparameters.get('triple2_units'))\n",
        "print('triple3_units: ', best_hyperparameters.get('triple3_units'))\n",
        "print('triple4_units: ', best_hyperparameters.get('triple4_units'))\n",
        "print('learning_rate: ', best_hyperparameters.get('learning_rate'))\n",
        "print('epsilon: ', best_hyperparameters.get('epsilon'))\n",
        "print('initial_accumulator_value: ', best_hyperparameters.get('initial_accumulator_value'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PkddpVikBFSz"
      },
      "source": [
        "### Restoring previously obtained best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C14O4nGR7OPc",
        "cellView": "code"
      },
      "source": [
        "hps = kt.HyperParameters()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGeTb9cd7QOi"
      },
      "source": [
        "hps.values = {\n",
        "    'layer1_units':  6,\n",
        "    'layer2_units':  8,\n",
        "    'triple1_units': 61, \n",
        "    'triple2_units': 123, \n",
        "    'triple3_units': 199, \n",
        "    'triple4_units': 288, \n",
        "    'learning_rate': 0.0018016,\n",
        "    'initial_accumulator_value': 0.00041422, \n",
        "    'epsilon': 6.687e-05\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5F6kyqloB-tS"
      },
      "source": [
        "Create model with the selected hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oHhqzntcivI"
      },
      "source": [
        "model = tuner.hypermodel.build(hps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6PfnFSMp9jW"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX4PkD9_sPLd"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E19eN1S7rwpx"
      },
      "source": [
        "Save trained model after each epoch using checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3v_Oj8ZGs5gT"
      },
      "source": [
        "model_count+=1\n",
        "path = model_path+str(model_count)\n",
        "print(path)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(path+'/checkpoint_{epoch:02d}', save_freq='epoch') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4fksz6R5wbC"
      },
      "source": [
        "epoch_no=100\n",
        "print('Training')\n",
        "H=model.fit(datagen.flow(x_train, y_train, batch_size=4), steps_per_epoch=x_train.shape[0],\n",
        "                      epochs=epoch_no, verbose=2, callbacks=[checkpoint, stop_early], \n",
        "                      validation_data=(x_val, y_val), workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZDmCw89JY3A"
      },
      "source": [
        "N = np.arange(0, len(H.history[\"val_loss\"]))\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.rcParams[\"figure.figsize\"] = (10,10)\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.title(\"Training and Validation Metrics\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend()\n",
        "plt.savefig(path+\"/plot.png\")\n",
        "plt.show(block=False)\n",
        "best = np.argmax(H[\"val_accuracy\"])\n",
        "output = \"Best model is from checkpoint #\" + str(best+1)\n",
        "output += \"\\nloss:{:.4f} accuracy:{:.4f} val_loss: {:.4f} val_accuracy:{:.4f}\".format(\n",
        "    H.history[\"loss\"][best], H.history[\"accuracy\"][best], H.history[\"val_loss\"][best], H.history[\"val_accuracy\"][best])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SevP3QOVrMYg"
      },
      "source": [
        "### Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfkUmqY4rOmj"
      },
      "source": [
        "print('Testing performance of model #{} with best validation accuracy'.format(best+1))\n",
        "model = tf.keras.models.load_model(path+'/checkpoint_{:02d}'.format(best))\n",
        "loss, acc = model.evaluate(x_test, y_test, batch_size=1,verbose=2)\n",
        "tf.keras.backend.clear_session()\n",
        "output += \"\\n\\nTest performance of the model at checkpoint #\" + str(best+1)\n",
        "output += \"\\nloss:{:.4f} accuracy:{:.4f}\".format(loss, acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQYoXD208WJh"
      },
      "source": [
        "Write complete output to a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvk7qpOunxrt"
      },
      "source": [
        "output += \"\\n\\nHistory:\\n\"+str(H.history)\n",
        "output += \"\\n\\nTest loss:{} Test accuracy:{}\".format(loss, acc)\n",
        "\n",
        "print('writing output to '+ path + '/output.txt\\n\\n')\n",
        "print(output)\n",
        "\n",
        "with open(path+\"/output.txt\", \"w\") as f:\n",
        "  f.write(output)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waRhasFrT3Lo"
      },
      "source": [
        "# Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXnkk8kbUi9n"
      },
      "source": [
        "Load saved model with best accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DANqI1PagRad"
      },
      "source": [
        "saved_model_dir = '/content/drive/MyDrive/main_project/models/saved_model_65/checkpoint_09'"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3hp9G4-T6Or"
      },
      "source": [
        "restored_model = keras.models.load_model(saved_model_dir)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9Aa1h0aYfbQ"
      },
      "source": [
        "Load and transform image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASJudHppUgSp"
      },
      "source": [
        "test_image_path = '/content/drive/MyDrive/main_project/Viral/Viral Pneumonia-10.png'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SO3K38hVKU4"
      },
      "source": [
        "img = cv2.imread(test_image_path)\n",
        "img = cv2.resize(img, (input_height, input_width), interpolation=cv2.INTER_CUBIC)\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # To convert colormap from BGR to GRAY\n",
        "#Normalize the image - convert the each pixel value between 0 and 1\n",
        "img = img / 255\n",
        "img = np.reshape(img, (1, input_height, input_width, n_channels))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfLK6_tlYtaH"
      },
      "source": [
        "Predict result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBmnBUBzYsrs",
        "outputId": "cc0e32a7-ff75-43f6-a0a0-1829a198f986"
      },
      "source": [
        "prediction = restored_model.predict(img)\n",
        "result = labels[np.argmax(prediction)]\n",
        "print(result)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Viral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5PjuHC3gHGU"
      },
      "source": [
        "# Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZGEiUeIgGRQ",
        "outputId": "02ecba26-cbc5-49f8-8398-38e7d1938e96"
      },
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(restored_model)\n",
        "tflite_model = converter.convert()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmphakbfj0u/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um9Zbv-DgjMJ"
      },
      "source": [
        "with open(\n",
        "    '/content/drive/MyDrive/main_project/tflite/model_1.0.tflite', \n",
        "    'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTJiVtZDtaeh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}